{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj9YcAnsT4B_"
      },
      "outputs": [],
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09b_0FAnUa9y"
      },
      "outputs": [],
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpA68lTrcvZs"
      },
      "outputs": [],
      "source": [
        "# initialize the Haar Cascade face detection model\n",
        "face_classifier = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwUtEojjnPRb"
      },
      "outputs": [],
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "\n",
        "  # get photo data\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  # get OpenCV format image\n",
        "  img = js_to_image(data)\n",
        "  # grayscale img\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  print(gray.shape)\n",
        "  # get face bounding box coordinates using Haar Cascade\n",
        "  faces = face_cascade.detectMultiScale(gray)\n",
        "  # draw face bounding box on image\n",
        "  for (x,y,w,h) in faces:\n",
        "      img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "  # save image\n",
        "  cv2.imwrite(filename, img)\n",
        "\n",
        "  return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilLkpcKanPRb"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  filename = take_photo('photo.jpg')\n",
        "  print('Saved to {}'.format(filename))\n",
        "\n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghUlAJzKSjFT"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "\n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "\n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "\n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "\n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "\n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "\n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "\n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML =\n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "\n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "\n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "\n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "\n",
        "      return {'create': preShow - preCreate,\n",
        "              'show': preCapture - preShow,\n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nkSnkbkk4cC"
      },
      "outputs": [],
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # grayscale image for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # get face region coordinates\n",
        "    faces = face_classifier.detectMultiScale(gray)\n",
        "    # get face bounding box for overlay\n",
        "    for (x,y,w,h) in faces:\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Start Here**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yvuiXdxEsioJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_i34YHXKuqq",
        "outputId": "a1b101ee-eb31-458b-a3d9-25d2f17f50a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TT8nRWHAK-Pq"
      },
      "outputs": [],
      "source": [
        "path='/content/drive/MyDrive/ColabNotebooks/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "Tssx96FuSxAZ",
        "outputId": "c08c8f2e-7b4f-44d9-84db-677dac89f6be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "NotReadableError: Device in use",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-da076528c8df>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Collecting samples is completed !!!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-da076528c8df>\u001b[0m in \u001b[0;36mgenerate_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# Capture photo from webcam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mtake_photo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temp.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temp.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-da076528c8df>\u001b[0m in \u001b[0;36mtake_photo\u001b[0;34m(filename, quality)\u001b[0m\n\u001b[1;32m     37\u001b[0m     ''')\n\u001b[1;32m     38\u001b[0m   \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'takePhoto({})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m   \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: NotReadableError: Device in use"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from google.colab.patches import cv2_imshow\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n",
        "def generate_dataset():\n",
        "    face_classifier = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
        "    def face_cropped(img):\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
        "        if len(faces)==0:\n",
        "            return None\n",
        "        for (x,y,w,h) in faces:\n",
        "            cropped_face = img[y:y+h,x:x+w]\n",
        "        return cropped_face\n",
        "    img_id = 0\n",
        "\n",
        "    while True:\n",
        "        # Capture photo from webcam\n",
        "        take_photo('temp.jpg')\n",
        "        frame = cv2.imread('temp.jpg')\n",
        "\n",
        "        if face_cropped(frame) is not None:\n",
        "            img_id += 1\n",
        "            face = cv2.resize(face_cropped(frame), (200,200))\n",
        "            face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "            file_name_path = \"/content/drive/MyDrive/ColabNotebooks/data/Name.\" + str(img_id) + \".jpg\"\n",
        "            # file_name_path = \"/content/drive/MyDrive/ColabNotebooks/imageforvisualisation\" + str(img_id) + \".jpg\"\n",
        "\n",
        "            cv2.imwrite(file_name_path, face)\n",
        "            cv2.putText(face, str(img_id), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
        "\n",
        "            # cv2.imshow(face)\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q') or img_id == 5:\n",
        "                break\n",
        "\n",
        "    cv2.destroyAllWindows()\n",
        "    print(\"Collecting samples is completed !!!\")\n",
        "\n",
        "generate_dataset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PJJmAVolRux"
      },
      "outputs": [],
      "source": [
        "import numpy as np # pip install numpy\n",
        "def my_label(image_name):\n",
        "    name = image_name.split('.')[-3]\n",
        "    #suppose your dataset contains two person\n",
        "#     if name==\"Ishwar\":\n",
        "#         return np.array([1,0])\n",
        "#     elif name==\"Manish\":\n",
        "#         return np.array([0,1])\n",
        "\n",
        "\n",
        "    # suppose your dataset contains three person\n",
        "    if name==\"Poorvi\":\n",
        "        return np.array([1,0,0,0])\n",
        "    elif name==\"Rhythm\":\n",
        "        return np.array([0,1,0,0])\n",
        "    elif name==\"harshit\":\n",
        "        return np.array([0,0,1,0])\n",
        "   # elif name==\"Amita\":\n",
        "    #    return np.array([0,0,0,1])\n",
        "    else:\n",
        "\n",
        "        return np.array([0,0,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WltTjb_ymNZ_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from random import shuffle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ip4cmeqmXwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d513b7b7-3763-44d5-85aa-62dd96152fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 138.71it/s]\n"
          ]
        }
      ],
      "source": [
        "def my_data():\n",
        "    data = []\n",
        "    for img in tqdm(os.listdir(\"/content/drive/MyDrive/ColabNotebooks/data\")):\n",
        "        path=os.path.join(\"/content/drive/MyDrive/ColabNotebooks/data\",img)\n",
        "        img_data = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "        img_data = cv2.resize(img_data, (50,50))\n",
        "        data.append([np.array(img_data), my_label(img)])\n",
        "    shuffle(data)\n",
        "    return data\n",
        "data=my_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK-Igu_ppF77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295e5cdb-613b-489c-d971-f0da7900f849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 50, 50, 1)\n",
            "(5, 50, 50, 1)\n"
          ]
        }
      ],
      "source": [
        "train = data[:10]\n",
        "test = data[10:]\n",
        "X_train = np.array([i[0] for i in train]).reshape(-1,50,50,1)\n",
        "print(X_train.shape)\n",
        "y_train = np.array([i[1] for i in train])\n",
        "X_test = np.array([i[0] for i in test]).reshape(-1,50,50,1)\n",
        "print(X_test.shape)\n",
        "y_test = np.array([i[1] for i in test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNJKT29Q29tM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "y_train = np.array(y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPxGY6M0vOlB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c7a25d-5972-4478-9c6e-02c9ae62b54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1/1 [==============================] - 1s 1s/step - loss: 34.9930 - accuracy: 0.4000 - val_loss: 132.2522 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/15\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 44.1356 - accuracy: 0.4000 - val_loss: 99.0255 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/15\n",
            "1/1 [==============================] - 0s 174ms/step - loss: 25.1672 - accuracy: 0.7000 - val_loss: 45.0491 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/15\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 23.4908 - accuracy: 0.5000 - val_loss: 4.3073 - val_accuracy: 0.4000\n",
            "Epoch 5/15\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 53.4951 - accuracy: 0.3000 - val_loss: 7.8334 - val_accuracy: 0.4000\n",
            "Epoch 6/15\n",
            "1/1 [==============================] - 0s 146ms/step - loss: 15.0172 - accuracy: 0.7000 - val_loss: 12.0203 - val_accuracy: 0.4000\n",
            "Epoch 7/15\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 11.6551 - accuracy: 0.4000 - val_loss: 8.7743 - val_accuracy: 0.4000\n",
            "Epoch 8/15\n",
            "1/1 [==============================] - 0s 154ms/step - loss: 18.3277 - accuracy: 0.4000 - val_loss: 1.6116 - val_accuracy: 0.4000\n",
            "Epoch 9/15\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.3942 - accuracy: 0.8000 - val_loss: 0.0089 - val_accuracy: 1.0000\n",
            "Epoch 10/15\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 1.4867 - accuracy: 0.8000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "Epoch 11/15\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.0519 - accuracy: 1.0000 - val_loss: 0.0540 - val_accuracy: 1.0000\n",
            "Epoch 12/15\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.0626 - accuracy: 1.0000 - val_loss: 0.3225 - val_accuracy: 0.8000\n",
            "Epoch 13/15\n",
            "1/1 [==============================] - 0s 157ms/step - loss: 0.4377 - accuracy: 0.9000 - val_loss: 0.7251 - val_accuracy: 0.6000\n",
            "Epoch 14/15\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 1.2592 - val_accuracy: 0.6000\n",
            "Epoch 15/15\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 1.3988e-04 - accuracy: 1.0000 - val_loss: 1.8123 - val_accuracy: 0.6000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dd4fba714e0>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "# Create a new Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add convolutional layers\n",
        "model.add(Conv2D(32, (5, 5), activation='relu', input_shape=(50, 50, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# model.add(Conv2D(128, (5, 5), activation='relu'))  # Commented out to avoid downsampling issue\n",
        "# model.add(MaxPooling2D(pool_size=(5, 5)))\n",
        "# model.add(Conv2D(64, (5, 5), activation='relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(5, 5 )))\n",
        "# model.add(Conv2D(32, (5, 5), activation='relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(5, 5)))\n",
        "\n",
        "# Flatten the output of the convolutional layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# Add fully connected layers\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.8))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgMjg3FZIN_V"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV6uAUq7oQdf"
      },
      "outputs": [],
      "source": [
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytz\n",
        "import datetime\n",
        "\n",
        "# Get the current time in the desired timezone (Asia/Kolkata for Rajasthan, India)\n",
        "timezone = pytz.timezone('Asia/Kolkata')\n",
        "now = datetime.datetime.now(tz=timezone)\n",
        "\n",
        "# Convert the current time to a string without timezone information\n",
        "current_time_str = now.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Now, you can write current_time_str to Excel without timezone information.\n"
      ],
      "metadata": {
        "id": "pqwKIty57L3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "from datetime import datetime\n",
        "from openpyxl import Workbook\n",
        "import numpy as np\n",
        "\n",
        "# Load the Haar cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Define the names of people whose data has been trained\n",
        "trained_names = [\"Poorvi\", \"Rhythm\",\"harshit\"]  # Add more names as per your dataset\n",
        "\n",
        "# Create a new Workbook\n",
        "wb = Workbook()\n",
        "\n",
        "# Select the active worksheet\n",
        "ws = wb.active\n",
        "\n",
        "# Write headers\n",
        "ws['A1'] = 'Date & Time'\n",
        "ws['B1'] = 'Name'\n",
        "\n",
        "# Define the function to write data to the Excel sheet\n",
        "def write_to_excel(name, timestamp):\n",
        "    row = [current_time_str, name]\n",
        "    ws.append(row)\n",
        "\n",
        "# Function to predict the name of the person\n",
        "def predict_name_with_timestamp(face, confidence_threshold=10):\n",
        "    # Preprocess the face\n",
        "    face = cv2.resize(face, (50, 50))\n",
        "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "    face = np.expand_dims(face, axis=0)\n",
        "    face = np.expand_dims(face, axis=-1)\n",
        "\n",
        "    # Predict using the model\n",
        "    prediction = model.predict(face)\n",
        "    name_index = np.argmax(prediction)\n",
        "\n",
        "    # Define the class labels corresponding to the indices\n",
        "    class_labels = [\"Poorvi\", \"Rhythm\", \"Amita\"]  # Add more names as per your dataset\n",
        "\n",
        "    # Check if the predicted name is in the list of trained names\n",
        "    predicted_name = class_labels[name_index]\n",
        "    if predicted_name in trained_names:\n",
        "        return predicted_name, datetime.now()\n",
        "    else:\n",
        "        return \"Unknown\", datetime.now()\n",
        "\n",
        "# Function to capture photo from webcam using JavaScript\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "          const div = document.createElement('div');\n",
        "          const capture = document.createElement('button');\n",
        "          capture.textContent = 'Capture';\n",
        "          div.appendChild(capture);\n",
        "\n",
        "          const video = document.createElement('video');\n",
        "          video.style.display = 'block';\n",
        "          const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "          document.body.appendChild(div);\n",
        "          div.appendChild(video);\n",
        "          video.srcObject = stream;\n",
        "          await video.play();\n",
        "\n",
        "          // Resize the output to fit the video element.\n",
        "          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "          // Wait for Capture to be clicked.\n",
        "          await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "          const canvas = document.createElement('canvas');\n",
        "          canvas.width = video.videoWidth;\n",
        "          canvas.height = video.videoHeight;\n",
        "          canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "          stream.getVideoTracks()[0].stop();\n",
        "          div.remove();\n",
        "          return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# Open the webcam and perform facial recognition\n",
        "camera = cv2.VideoCapture(0)\n",
        "while True:\n",
        "    try:\n",
        "        # Capture photo from webcam\n",
        "        take_photo('temp.jpg')\n",
        "        frame = cv2.imread('temp.jpg')\n",
        "\n",
        "        # Convert the frame to grayscale for face detection\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Detect faces in the frame\n",
        "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "        # Iterate over detected faces\n",
        "        for (x, y, w, h) in faces:\n",
        "            # Extract the detected face\n",
        "            face = frame[y:y + h, x:x + w]\n",
        "\n",
        "            # Predict the name of the person\n",
        "            name, timestamp = predict_name_with_timestamp(face)\n",
        "\n",
        "            # Write the predicted name along with the date and time to Excel sheet\n",
        "            write_to_excel(name, timestamp)\n",
        "\n",
        "            # Draw a rectangle around the detected face and display the name\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "            cv2.putText(frame, name, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
        "\n",
        "        # Display the frame\n",
        "        cv2_imshow(frame)\n",
        "\n",
        "        # Check for the 'q' key to quit the loop\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        break\n",
        "\n",
        "# Release the camera and close all windows\n",
        "camera.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Define the file path in Google Drive\n",
        "file_path = \"/content/drive/My Drive/facial_recognition_data.xlsx\"\n",
        "\n",
        "# Save the workbook to Google Drive\n",
        "wb.save(file_path)\n",
        "\n",
        "print(f\"Data has been written to {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHNz5-nun4hN",
        "outputId": "29deaa52-c2ab-440d-b203-660614e66d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'NoneType' object has no attribute 'split'\n",
            "Data has been written to /content/drive/My Drive/facial_recognition_data.xlsx\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}